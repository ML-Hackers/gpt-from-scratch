model_name: gpt2-large
max_seq_length: 1024
num_train_epochs: 3
max_steps: -1
logging_steps: 10
eval_steps: 100
bf16: True
packing: False
per_device_train_batch_size: 8
gradient_accumulation_steps: 16
dataset_text_field: "text"
use_gradient_checkpointing: False
use_flash_attn: False
dataset_name: timdettmers/openassistant-guanaco
#dataset_name: wikimedia/wikipedia
dataset_subset: 20231101.en
num_workers: 16
dataset_filter: 1000000
lr_scheduler_type: cosine
learning_rate: 0.001
